{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Wrangling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required package\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'default of credit card clients.xls'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-fb75fbf88f80>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m# we skipped the first row because the data had two rows with column labels\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;31m#xl = pd.read_excel(url, sheet_name=None, skiprows = [0])\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[0mxl\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_excel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'default of credit card clients.xls'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mskiprows\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\util\\_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    176\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    177\u001b[0m                     \u001b[0mkwargs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnew_arg_name\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnew_arg_value\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 178\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    179\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    180\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0m_deprecate_kwarg\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\util\\_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    176\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    177\u001b[0m                     \u001b[0mkwargs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnew_arg_name\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnew_arg_value\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 178\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    179\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    180\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0m_deprecate_kwarg\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\excel.py\u001b[0m in \u001b[0;36mread_excel\u001b[1;34m(io, sheet_name, header, names, index_col, usecols, squeeze, dtype, engine, converters, true_values, false_values, skiprows, nrows, na_values, parse_dates, date_parser, thousands, comment, skipfooter, convert_float, **kwds)\u001b[0m\n\u001b[0;32m    305\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    306\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mio\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mExcelFile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 307\u001b[1;33m         \u001b[0mio\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mExcelFile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mio\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    308\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    309\u001b[0m     return io.parse(\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\excel.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, io, **kwds)\u001b[0m\n\u001b[0;32m    392\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbook\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mxlrd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen_workbook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile_contents\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    393\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_io\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstring_types\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 394\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbook\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mxlrd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen_workbook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_io\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    395\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    396\u001b[0m             raise ValueError('Must explicitly set engine if not passing in'\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\xlrd\\__init__.py\u001b[0m in \u001b[0;36mopen_workbook\u001b[1;34m(filename, logfile, verbosity, use_mmap, file_contents, encoding_override, formatting_info, on_demand, ragged_rows)\u001b[0m\n\u001b[0;32m    114\u001b[0m         \u001b[0mpeek\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfile_contents\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mpeeksz\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    115\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 116\u001b[1;33m         \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"rb\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    117\u001b[0m             \u001b[0mpeek\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpeeksz\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    118\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mpeek\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34mb\"PK\\x03\\x04\"\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;31m# a ZIP file\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'default of credit card clients.xls'"
     ]
    }
   ],
   "source": [
    "# Assign url of file: url\n",
    "url = 'https://archive.ics.uci.edu/ml/machine-learning-databases/00350/default%20of%20credit%20card%20clients.xls'\n",
    "\n",
    "# Read in all sheets of Excel file and load into xl \n",
    "# we skipped the first row because the data had two rows with column labels\n",
    "#xl = pd.read_excel(url, sheet_name=None, skiprows = [0])\n",
    "xl = pd.read_excel('default of credit card clients.xls', skiprows = [0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xl.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert the xl OrderedDict to a pandas dataframe\n",
    "#df = pd.DataFrame(xl['Data'])\n",
    "df = xl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check the dataframe\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a new list of column label to rename the columns of dataframe\n",
    "new_columns = ['id','limit_balance', 'gender', 'education','marital_status', 'age','status_september','status_august','status_july','status_june','status_may','status_april','balance_september','balance_august','balance_july','balance_june','balance_may','balance_april','paid_september','paid_august','paid_july','paid_june','paid_may','paid_april','default_payment']\n",
    "#change the column labels\n",
    "df.columns = new_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#to check the column labels\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#to get information about the data\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using the boxplot to check for outliers\n",
    "df.boxplot('age',figsize=(10,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a list of column labels for different months for balance amount and paid amount\n",
    "balance = ['balance_september','balance_august','balance_july','balance_june','balance_may','balance_april']\n",
    "paid = ['paid_september','paid_august','paid_july','paid_june','paid_may','paid_april']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we can use boxplot to check for outliers from balance columns for each month\n",
    "df.boxplot(balance, figsize=(10,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Draw boxplot to detect outliers from paid column for each month\n",
    "df.boxplot(paid, figsize=(10,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#find min, max, median, 1st and 3rd quartile values for balance column of each month \n",
    "df[balance].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#find min, max, median, 1st and 3rd quartile values for paid column of each month\n",
    "df[paid].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# I created a new dataframe to update and modify bad data plus the outliers values. I   \n",
    "The bad data are replaced with their best possible values.\n",
    "\n",
    "The outliers are updated with their closest lower and upper quartile range value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#to check for bad data in the education column\n",
    "new_df[new_df['education']>4]['education'].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df[new_df['education']<1]['education'].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#to change the bad values 5, 6 and 0 into 4 from education column\n",
    "for i in new_df[(new_df['education']>4)|(new_df['education']<1)]['education'].index:\n",
    "    new_df.at[i,'education'] = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#to check if there is any bad data in the marital_status column\n",
    "new_df[(new_df['marital_status']<1)|(new_df['marital_status']>3)]['marital_status'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#to change the bad value 0 from marital_status column\n",
    "for i in new_df[new_df['marital_status']==0]['marital_status'].index:\n",
    "    new_df.at[i,'marital_status'] = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#to check if there is any bad data in the status columns, looking at the min and max values\n",
    "status=['status_september','status_august','status_july','status_june','status_may','status_april']\n",
    "new_df[status].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#to check what was the client's status was before it changed to -2\n",
    "#we can see that the client's pay status was -1 before it changed to -1.\n",
    "#we will change the value from -2 to -1 \n",
    "new_df[(new_df['status_september']<-1)|(new_df['status_september']==0)][status]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#to change the bad values -2 and 0 into -1 from status column\n",
    "for month in status:\n",
    "    for i in new_df[(new_df[month]<-1)|(new_df[month]==0)][month].index:\n",
    "        new_df.at[i,month] = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this for loop will locate the outliers from the dataframe and update those values with their corresponding lower and upper limit\n",
    "#iterate over the list named balance, which has the balance column labels\n",
    "for month in balance:\n",
    "    #calculate 3rd quartile\n",
    "    q3 = new_df[month].quantile(0.75)\n",
    "    #calculate 1st quartile\n",
    "    q1 = new_df[month].quantile(0.25)\n",
    "    #calculate interquartile range\n",
    "    iqr = q3-q1\n",
    "    lower = q1-(1.5*iqr)\n",
    "    upper = q3+(1.5*iqr)\n",
    "    lower_index = new_df[new_df[month]<=lower].index\n",
    "    upper_index = new_df[new_df[month]>=upper].index\n",
    "    for i in lower_index:\n",
    "        new_df.at[i,month] = lower\n",
    "    for i in upper_index:\n",
    "        new_df.at[i,month] = upper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check to if the vlaues has been updated\n",
    "new_df[balance].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max(new_df[balance].max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min(new_df[balance].min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this for loop will locate the outliers from the dataframe and update those values with their corresponding lower and upper limit\n",
    "#iterate over the list named paid, which has the paid column labels\n",
    "for month in paid:\n",
    "    #calculate 3rd quartile\n",
    "    q3 = new_df[month].quantile(0.75)\n",
    "    #calculate 1st quartile\n",
    "    q1 = new_df[month].quantile(0.25)\n",
    "    #calculate interquartile range\n",
    "    iqr = q3-q1\n",
    "    lower = q1-(1.5*iqr)\n",
    "    upper = q3+(1.5*iqr)\n",
    "    lower_index = new_df[new_df[month]<=lower].index\n",
    "    upper_index = new_df[new_df[month]>=upper].index\n",
    "    for i in lower_index:\n",
    "        new_df.at[i,month] = lower\n",
    "    for i in upper_index:\n",
    "        new_df.at[i,month] = upper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#to check the update\n",
    "new_df[paid].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max(new_df[paid].max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#to check the updated value using boxplot\n",
    "new_df.boxplot(balance,figsize=(10,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#to check the updated values in those column after dealing with outliers\n",
    "new_df.boxplot(paid, figsize=(10,5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Story"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#to find out total number of 1 default and 0 non-default \n",
    "new_df['default_payment'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#to calculate the percentage of 1 default and 0 non-default\n",
    "default_rate = new_df['default_payment'].value_counts() * 100 / len(new_df)\n",
    "default_rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# After we wrangled and cleaned the dataset, we started to explore the data in detail. The first step was to see the count and distributions of different variables from the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ● How many cardholders are defaulters?\n",
    "We found that the  77.88% of the cardholders(23,364) did not default and 22.12% of the cardholders (6,636) default."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#draw a bar plot to visualize the default payment\n",
    "import matplotlib.patches as mpatches\n",
    "plt.subplot(1,2,1)\n",
    "default_rate.plot(kind='bar',title='Percentage Distribution of Default Payment',figsize=(10,5))\n",
    "ND = mpatches.Patch(color ='blue', label = '0-Non Default')\n",
    "DT = mpatches.Patch(color ='orange', label = '1-Default')\n",
    "plt.legend(handles=[ND, DT], loc=0)\n",
    "plt.xlabel('default status')\n",
    "plt.ylabel('Population Percentage')\n",
    "plt.subplot(1,2,2)\n",
    "new_df['default_payment'].value_counts().plot(kind='bar',title='Count of Default Payment')\n",
    "plt.xlabel('default_status')\n",
    "plt.ylabel('Number of card-holder')\n",
    "ND = mpatches.Patch(color ='blue', label = '0-Non Default')\n",
    "DT = mpatches.Patch(color ='orange', label = '1-Default')\n",
    "plt.legend(handles=[ND, DT], loc=0)\n",
    "plt.tight_layout()\n",
    "plt.savefig('Default_Payment_distribution.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#to find out total number of 1 male and 2 female\n",
    "#there is more 2 female \n",
    "new_df['gender'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate the gender distribution\n",
    "gender_rate = new_df['gender'].value_counts() * 100 / len(new_df)\n",
    "gender_rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ● How are the cardholders divided by gender? \n",
    "We have 60% female and 40% male clients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#to calculate the percentage and count of gender\n",
    "plt.subplot(1,2,1)\n",
    "#draw a bar plot to see the total number of 1 male and 2 female card holder\n",
    "gender_rate.plot(kind='bar',title='Percentage Distribution of Gender',figsize=(10,5))\n",
    "ML = mpatches.Patch(color ='orange', label = '1-Male')\n",
    "FM = mpatches.Patch(color ='blue', label = '2-Female')\n",
    "plt.legend(handles=[ML, FM], loc=0)\n",
    "plt.ylabel('Population Percentage')\n",
    "plt.subplot(1,2,2)\n",
    "new_df.gender.value_counts().plot(kind='bar',title='Count of Gender')\n",
    "ML = mpatches.Patch(color ='orange', label = '1-Male')\n",
    "FM = mpatches.Patch(color ='blue', label = '2-Female')\n",
    "plt.legend(handles=[ML, FM], loc=0)\n",
    "plt.ylabel('Number of Card-holder')\n",
    "plt.tight_layout()\n",
    "plt.savefig('gender_distribution.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#to find out the total number of cardholder with different education level\n",
    "#2 University level is high, and 4 others is low\n",
    "new_df['education'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#to calculate the percentage education distribution\n",
    "education_rate = new_df['education'].value_counts() * 100 / len(new_df)\n",
    "education_rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ● What are the education level, and Which education level does the most of the cardholders belong to? \n",
    "Most of our cardholder have University level education for their highest level of education. We have 35% with Graduate level education, 46% with University level, 16% with High School level, and 1.5% with Others as level of education."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#draw a bar plot to see what education level most of the card holder have\n",
    "#University level  is the highest\n",
    "plt.subplot(1,2,1)\n",
    "education_rate.plot(kind='bar',title='Percentage Distribution of Education Level',figsize=(10,5))\n",
    "GD = mpatches.Patch(color ='Orange', label = '1-Graduate')\n",
    "UN = mpatches.Patch(color ='blue', label = '2-University')\n",
    "HS = mpatches.Patch(color ='green', label = '3-High School')\n",
    "OT = mpatches.Patch(color ='red', label = '4-Other')\n",
    "plt.legend(handles=[GD, UN, HS, OT], loc=0)\n",
    "plt.ylabel('Population Percentage')\n",
    "plt.subplot(1,2,2)\n",
    "new_df.education.value_counts().plot(kind='bar',title='Count of Education Level',figsize=(10,5))\n",
    "GD = mpatches.Patch(color ='Orange', label = '1-Graduate')\n",
    "UN = mpatches.Patch(color ='blue', label = '2-University')\n",
    "HS = mpatches.Patch(color ='green', label = '3-High School')\n",
    "OT = mpatches.Patch(color ='red', label = '4-Other')\n",
    "plt.legend(handles=[GD, UN, HS, OT], loc=0)\n",
    "plt.ylabel('Number of Card-holder')\n",
    "plt.tight_layout()\n",
    "plt.savefig('education_distribution.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#to find out the count of different martial status,1 married, 2 single, 3 others from the dataset \n",
    "#2 i.e single is highest\n",
    "new_df['marital_status'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#to calculate the percentage education distribution\n",
    "marital_status_rate = new_df['marital_status'].value_counts() * 100 / len(new_df)\n",
    "marital_status_rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ● How many cardholders are married and how many are single?\n",
    "We have 53% married, 45% single, and rest as others."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplot(1,2,1)\n",
    "#draw a bar plot to see the population of married, single, and others\n",
    "#single is highest, married is second and others is lowest\n",
    "marital_status_rate.plot(kind='bar', title='Percentage Distribution of Marital_Status',figsize=(10,5))\n",
    "MR = mpatches.Patch(color ='orange', label = '1-Married')\n",
    "SG = mpatches.Patch(color ='blue', label = '2-Single')\n",
    "O = mpatches.Patch(color ='green', label = '3-Other')\n",
    "plt.legend(handles=[MR, SG, O], loc=0)\n",
    "plt.ylabel('Population Percentage')\n",
    "plt.subplot(1,2,2)\n",
    "new_df['marital_status'].value_counts().plot(kind='bar', title='Count of Marital_Status',figsize=(10,5))\n",
    "MR = mpatches.Patch(color ='orange', label = '1-Married')\n",
    "SG = mpatches.Patch(color ='blue', label = '2-Single')\n",
    "O = mpatches.Patch(color ='green', label = '3-Other')\n",
    "plt.legend(handles=[MR, SG, O], loc=0)\n",
    "plt.ylabel('Number of Card-holder')\n",
    "plt.tight_layout()\n",
    "plt.savefig('marital_status_distribution.jpg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ● What age group is the majority of the cardholders?\n",
    "Most of our carholders are of age group 20 to 40. This exploration can provide us with the demographic of different variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we can also draw a histogram to see our age group in the dataset\n",
    "plt.subplot(1,2,1)\n",
    "new_df['age'].hist(bins=6,figsize=(10,5),normed=1,grid=False)\n",
    "plt.title('Distribution of Age')\n",
    "plt.xlabel('age')\n",
    "plt.ylabel('Population')\n",
    "plt.subplot(1,2,2)\n",
    "new_df['age'].hist(bins=6, grid=False)\n",
    "plt.title('Count of Age-Group')\n",
    "plt.xlabel('age')\n",
    "plt.ylabel('Number of Card-holder')\n",
    "plt.tight_layout()\n",
    "plt.savefig('age_histogram.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#group the data by gender and find out the default rate\n",
    "total_defaults = np.sum(new_df['default_payment']==1)\n",
    "group_gender = new_df.groupby('gender')['default_payment'].sum().reset_index(name='default')\n",
    "group_gender"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The main focus of this project is to create different Machine Learning Models to predict default, so let's find out some insights using different data visualization. We drew barplots to compare different variables with the default cardholders. From each plot, we learned which sub-variable effects default."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now we will commpare default rate with Gender\n",
    "#it shows that Female is the lowest to default\n",
    "#married and single both are high in number to default\n",
    "group_gender.plot(kind='bar',x='gender', y='default',title='Gender with Default',figsize=(10,5))\n",
    "ML = mpatches.Patch(color ='blue', label = '1-Male')\n",
    "FM = mpatches.Patch(color ='orange', label = '2-Female')\n",
    "plt.legend(handles=[ML, FM], loc=0)\n",
    "plt.ylabel('default')\n",
    "plt.tight_layout()\n",
    "plt.savefig('gender_default_bar.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_education = new_df.groupby(['education'])['default_payment'].sum().reset_index(name='default')\n",
    "group_education"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# We found that University level cardholders default more than other education level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now we will commpare default with all marital status\n",
    "#it shows that others is the lowest to default\n",
    "#married and single both are high in number to default\n",
    "group_education.plot(kind='bar',x='education',y='default',figsize=(10,5),title='Education Level with Default')\n",
    "G = mpatches.Patch(color ='blue', label = '1-Graduate')\n",
    "U = mpatches.Patch(color ='orange', label = '2-University')\n",
    "H = mpatches.Patch(color ='green', label = '3-High School')\n",
    "OT = mpatches.Patch(color ='red', label = '4-Other')\n",
    "plt.legend(handles=[G, U, H, OT], loc=0)\n",
    "plt.ylabel('default')\n",
    "plt.tight_layout()\n",
    "plt.savefig('education_default_bar.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#group the data by marital_status to see who has maximum default\n",
    "group_marital = new_df.groupby(['marital_status'])['default_payment'].sum().reset_index(name='default')\n",
    "group_marital"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# From marital_status with default plot, we found that both married and single have very close number of default, and others have a very low default."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now we will commpare default with all marital status\n",
    "#it shows that others is the lowest to default\n",
    "#married and single both are high in number to default\n",
    "group_marital.plot(kind='bar',x='marital_status',y='default',title='Marital Status with Default',figsize=(10,5))\n",
    "MR = mpatches.Patch(color ='blue', label = '1-Married')\n",
    "SG = mpatches.Patch(color ='orange', label = '2-Single')\n",
    "O = mpatches.Patch(color ='green', label = '3-Other')\n",
    "plt.legend(handles=[MR, SG, O], loc=0)\n",
    "plt.ylabel('default')\n",
    "plt.tight_layout()\n",
    "plt.savefig('marital_status_default_bar.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#to work with age group, create a list of age group to use as a bins\n",
    "#create a new column with column label 'age_bin'\n",
    "bins = [20,30,40,50,60,70,80]\n",
    "new_df['age_bin']= pd.cut(new_df['age'], bins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#find out the population if different age group\n",
    "#age group 20 to 30 is the highest, and 30-40 is the second highest\n",
    "new_df['age_bin'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Find the percentage of different age_group\n",
    "agegroup_rate = new_df['age_bin'].value_counts() * 100 / len(new_df)\n",
    "agegroup_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#now group the data by age_bin\n",
    "group_age = new_df.groupby('age_bin')['default_payment'].sum().reset_index(name='default')\n",
    "group_age"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Age group with default shows that age group 20 to 30 default more than any other age-group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now,default_payment with age groups\n",
    "#more number of age-group(20-30) clients default\n",
    "group_age.plot(kind='bar',x='age_bin',y='default',title='Age Group with Default',figsize=(10,5))\n",
    "plt.ylabel('default')\n",
    "plt.tight_layout()\n",
    "plt.savefig('age_group_default.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#now group the data by age to find what age is the majority of defaulter\n",
    "group_age1 = new_df.groupby('age')['default_payment'].sum().reset_index(name='default')\n",
    "group_age1.sort_values(by=\"default\", ascending=False).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now,default_payment with age groups\n",
    "#more number of age-group(20-30) clients default\n",
    "group_age1.plot(kind='bar',x='age',y='default',title='Age with Default',figsize=(10,5))\n",
    "plt.ylabel('default')\n",
    "plt.tight_layout()\n",
    "plt.savefig('age_default.jpg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# We found that some cardholders have negative balance. This means that some cardholders are paying the bank more than their balance or some transaction of purchase may have been refunded to the credit card."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#to check the updated value using boxplot\n",
    "new_df.boxplot(balance,figsize=(10,5))\n",
    "plt.savefig('balance_box.jpg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# When we compared balance column with paid column for the month of september, we saw that there is a positive linear relationship between the balance and paid columns except for some balances. Looking at the plot, those balances may be paid by automatic payment every month."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = sns.FacetGrid(new_df, row='education', col=\"default_payment\", hue='gender', size=4)\n",
    "g.map(plt.scatter,  \"balance_september\", \"paid_september\", alpha=0.5, edgecolor='k', linewidth=0.5, s=new_df['marital_status']*10)\n",
    "fig = g.fig \n",
    "fig.set_size_inches(20, 20)\n",
    "fig.subplots_adjust(top=0.85, wspace=0.3)\n",
    "fig.suptitle('Paid_september - balance_september - Education - default_payment - gender - marital_status', fontsize=14)\n",
    "l = g.add_legend(title='Gender')\n",
    "plt.savefig('paid_balance_scatter.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#group the data by marital_status and gender to see who has maximum default\n",
    "group_marital_gender = new_df.groupby(['marital_status','gender'])['default_payment'].sum().reset_index(name='default')\n",
    "group_marital_gender"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# From the analysis and visualizing the dataset, we found that the maximum number of defaulter are female, with University level education, and age between 20 and 30."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,5))\n",
    "#draw a barplot to check if female has the higher default rate\n",
    "sns.barplot(x='marital_status',y='default',hue='gender',data=group_marital_gender)\n",
    "plt.title('Marital Status and Gender')\n",
    "plt.tight_layout()\n",
    "plt.savefig('marital_gender_default.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#now group the data by age_bin and gender\n",
    "group_age_gender = new_df.groupby(['age_bin','gender'])['default_payment'].sum().reset_index(name='default')\n",
    "group_age_gender"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# When we compared male and female for each age group with default, we saw that more female of age group 20-30 default more, and both older male and older female after age group 50-80 default less."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,5))\n",
    "sns.barplot(x='age_bin',y='default',hue='gender',data=group_age_gender)\n",
    "plt.title('Age Group and Gender')\n",
    "plt.tight_layout()\n",
    "plt.savefig('age_group_gender_default.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_gender_education = new_df.groupby(['gender','education'])['default_payment'].sum().reset_index(name='default')\n",
    "group_gender_education.set_index(['gender','education'],inplace=True)\n",
    "group_gender_education.sort_values('default',ascending=False).head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# When we group gender and education column, we see that 2,2 or Female with University level education has the maximum default."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_gender_education['default'].plot(kind='bar',title='Gender and Education with Total Population',figsize=(10,5))\n",
    "plt.ylabel('Default')\n",
    "plt.tight_layout()\n",
    "plt.savefig('gender_education_default.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_gender_education_marital = new_df.groupby(['gender','education','marital_status'])['default_payment'].sum().reset_index(name='default')\n",
    "group_gender_education_marital.set_index(['gender','education','marital_status'],inplace=True)\n",
    "group_gender_education_marital.sort_values('default',ascending=False).head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# When we compared gender, education and marital status with default, we found that a female, university level, and married cardholder has the maximum default."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_gender_education_marital['default'].plot(kind='bar',title='Combined Gender, Education, Marital Status',figsize=(10,5))\n",
    "plt.ylabel('Default')\n",
    "plt.tight_layout()\n",
    "plt.savefig('gender_education_marital_default.jpg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inferential statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create two dataframes: m for male and f for female \n",
    "m = new_df[new_df.gender==1]\n",
    "f = new_df[new_df.gender==2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#t-statistics\n",
    "#H0: p(m.default_payment) = p(pop_f.default_payment)\n",
    "#Ha: p(m.default_payment) != p(pop_f.default_payment)\n",
    "\n",
    "# calculate the sample mean of male and female sample default_payment\n",
    "sample_p_m = np.mean(m.default_payment)\n",
    "sample_p_f = np.mean(f.default_payment)\n",
    "# calculate the difference of sample mean of male and female\n",
    "diff_p = sample_p_m - sample_p_f\n",
    "# calculate the size of sample male and female\n",
    "n_m = len(m.default_payment)\n",
    "n_f = len(f.default_payment)\n",
    "# calculate the variance of male and female\n",
    "variance_m = np.var(m.default_payment)\n",
    "variance_f = np.var(f.default_payment)\n",
    "#Calculate the t-value\n",
    "t= diff_p / np.sqrt((variance_m/n_m)+(variance_f/n_f))\n",
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate the degree of freedom\n",
    "dof = n_m + n_f -2\n",
    "dof"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We assume equal population variance\n",
    "standard_error = np.sqrt(((n_m-1)*variance_m + (n_f-1)*variance_f) / (n_m+n_f-2))*(np.sqrt((1/n_m)+(1/n_f)))\n",
    "#the critical t-value is 1.960 for degree of freedom 29998 and alpha 0.05\n",
    "margin_of_error = 1.96* standard_error\n",
    "margin_of_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate the confidence interval for 95%\n",
    "conf_int = [diff_p - margin_of_error, diff_p + margin_of_error]\n",
    "conf_int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "\n",
    "#calculation p-value for 2-tailed test. \n",
    "p_value = stats.t.sf(t, dof)*2\n",
    "print('p-value is ',p_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The p-value is less than level of significance 0.05, so we reject the null hypothesis. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, use the bootstrap method\n",
    "#Bootstrap replicate function to resample data and find mean/std\n",
    "def bootstrap_replicate_1d(data, func):\n",
    "    return func(np.random.choice(data, size=len(data)))\n",
    "\n",
    "#draw many bootstrap replicates \n",
    "def draw_bs_reps(data, func, size=1):\n",
    "    return np.array([bootstrap_replicate_1d(data, func) for _ in range(size)])\n",
    "\n",
    "mean_diff = np.mean(m.default_payment) - np.mean(f.default_payment)\n",
    "#get bootstrap replicates of data sets\n",
    "bs_replicates_m = draw_bs_reps(m.default_payment, np.mean, size=10000)\n",
    "bs_replicates_f = draw_bs_reps(f.default_payment, np.mean, size=10000)\n",
    "#compute replicates of difference of means: bs_diff_replicates\n",
    "bs_diff_replicates = bs_replicates_m - bs_replicates_f\n",
    "conf_interval = np.percentile(bs_diff_replicates, [2.5, 97.5])\n",
    "conf_interval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To find the p-value\n",
    "combined_mean = np.mean(new_df.default_payment)\n",
    "#shift the samples\n",
    "m_shifted = m.default_payment - np.mean(m.default_payment) + combined_mean\n",
    "f_shifted = f.default_payment - np.mean(f.default_payment) + combined_mean\n",
    "#get bootstrap replicates of shifted data sets\n",
    "bs_replicates_m = draw_bs_reps(m_shifted, np.mean, size=10000)\n",
    "bs_replicates_f = draw_bs_reps(f_shifted, np.mean, size=10000)\n",
    "#compute replicates of difference of means:\n",
    "bs_diff_replicates = bs_replicates_m - bs_replicates_f\n",
    "#compute the p-value\n",
    "p = np.sum(bs_diff_replicates >= mean_diff) / len(bs_diff_replicates)\n",
    "p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'''P-value is less than 0.05, we will reject the null hypothesis. z-score 6.921 is more extreme than the threshold of 1.96, so we will reject the null hypothesis. The gender is one important factor for default payment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chi-squared Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# For: Gender\n",
    "\n",
    "H0: Gender and default_payment are independent.\n",
    "\n",
    "Ha: Gender and default_payment are dependent.  \n",
    "\n",
    "alpha = 0.05\n",
    "\n",
    "df = (rows-1)*(columns-1)\n",
    "df = (2-1) * (2-1)\n",
    "df = 1\n",
    "\n",
    "critical chi squared value = 3.84146"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chi_squared_test = new_df.groupby(['default_payment','gender']).count()['id'].unstack()\n",
    "chi_squared_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "                  Gender\t\tMale\t\t    Female\t\tTotal\n",
    "\n",
    "         default_Payment\t\n",
    "\n",
    "                     No\t\t9015\t\t    14349\t\t23364 (0.7788)\n",
    "\n",
    "            Expected NO\t    9258.37\t     14105.62\n",
    "            \n",
    "                    Yes\t\t2873\t\t    3763\t\t6636 (0.2212)\n",
    "\n",
    "           Expected Yes\t   2629.62          4006.37\n",
    "\n",
    "                  Total\t\t11888 (0.3963)\t18112 (0.6037)\t30000\n",
    "\n",
    "level of significance = 0.05\n",
    "\n",
    "Chi squared value = (9015-9258.37)squared/9258.37 + (14349-14105.62)squared/14105.62 + (2873-2629.62)squared/2629.62 + (3763-4006.37)squared/4006.37\n",
    "\n",
    "= 47.90\n",
    "\n",
    "DF = 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import chisquare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "observed = [[9015, 14349], [2873, 3763]]\n",
    "chi2, p, dof, expected = stats.chi2_contingency(observed)\n",
    "print('chi2:',chi2)\n",
    "print('DF:',dof)\n",
    "print('p-value:',p)\n",
    "print(expected)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The chi squared statistics is 47.71\n",
    "P-value is <0.001\n",
    "The result is significant at p<0.05\n",
    "We reject the null hypothesis and suggest the alternative hypothesis.\n",
    "The default_payment is dependent on gender."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# For Education:\n",
    "\n",
    "H0: Education and default_payment are not related.\n",
    "\n",
    "Ha: Education and default_payment are related. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chi_squared_test_education = new_df.groupby(['default_payment','education']).count()['id'].unstack()\n",
    "chi_squared_test_education"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs = [[8549,10700,3680,435],[2036,3330,1237,33]]\n",
    "chi2, p, dof, expected = stats.chi2_contingency(obs)\n",
    "print('chi2:',chi2)\n",
    "print('DF:',dof)\n",
    "print('p-value:',p)\n",
    "print(expected)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The chi squared test p-value is < 0.001\n",
    "The result is significant at p < 0.05\n",
    "We reject the null hypothesis and suggest the alternative hypothesis.\n",
    "The default_payment is dependent on education. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# For Marital_status:\n",
    "\n",
    "H0: Marital_status and default_payment are not related.\n",
    "\n",
    "Ha: Marital_status and default_payment are related."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chi_squared_test_marital_status = new_df.groupby(['default_payment','marital_status']).count()['id'].unstack()\n",
    "chi_squared_test_marital_status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs = [[10453,12623,288],[3206,3341,89]]\n",
    "chi2, p, dof, expected = stats.chi2_contingency(obs)\n",
    "print('chi2:',chi2)\n",
    "print('DF:',dof)\n",
    "print('p-value:',p)\n",
    "print(expected)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The chi squared test p-value is < 0.001\n",
    "The result is significant at p < 0.05\n",
    "We reject the null hypothesis and suggest the alternative hypothesis.\n",
    "The default_payment is dependent on marital_status."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # For Age-group:\n",
    "\n",
    "H0: Age-group and default_payment are not related.\n",
    "\n",
    "Ha: Age-group and default_payment are related."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chi_squared_test_age = new_df.groupby(['default_payment','age_bin']).count()['id'].unstack()\n",
    "chi_squared_test_age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs = [[8542,8524,4606,1493,189,10],[2471,2189,1399,504,68,5]]\n",
    "chi2, p, dof, expected = stats.chi2_contingency(obs)\n",
    "print('chi2:',chi2)\n",
    "print('DF:',dof)\n",
    "print('p-value:',p)\n",
    "print(expected)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The chi squared test p-value is < 0.001\n",
    "The result is significant at p < 0.05\n",
    "We reject the null hypothesis and suggest the alternative hypothesis.\n",
    "The default_payment is dependent on age."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic Regression\n",
    "\n",
    "Decision Tree\n",
    "\n",
    "Gaussian Naive Bayes Classifier\n",
    "\n",
    "Random Forest Classifier\n",
    "\n",
    "Support Vector Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create dummies features for all the categorical columns data\n",
    "new_df = pd.get_dummies(new_df, columns=['gender','education', 'marital_status'], drop_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for status of the client\n",
    "pay_features = ['status_september','status_august','status_july','status_june', 'status_may','status_april',]\n",
    "for p in pay_features:\n",
    "    new_df.loc[new_df[p]<=0, p] = 0\n",
    "    \n",
    "new_df = new_df.drop('age_bin', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import necessary packages from scikit learn\n",
    "\n",
    "#Import LogisticRegression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "#Import DecisionTree\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "#Import GaussianNB\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "#Import RandomForest Classifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "#Import Support Vector Machine\n",
    "from sklearn import svm\n",
    "\n",
    "# import cross_val_score to evaluate the score by cross-validation\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# import train_test_split to split data into training and testing set\n",
    "from sklearn.model_selection import train_test_split\n",
    "# import different metrics we will use to evaluate the models\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, confusion_matrix, roc_curve, auc\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import classification_report\n",
    "from collections import Counter\n",
    "from imblearn.under_sampling import RandomUnderSampler, NearMiss, CondensedNearestNeighbour\n",
    "from imblearn.under_sampling import EditedNearestNeighbours, RepeatedEditedNearestNeighbours, TomekLinks\n",
    "from imblearn.over_sampling import RandomOverSampler, SMOTE\n",
    "from imblearn.combine import SMOTEENN, SMOTETomek\n",
    "from imblearn.ensemble import BalanceCascade, EasyEnsemble\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "import warnings\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from time import time\n",
    "from scipy.stats import randint as sp_randint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X is the features/data we use for our model (input data)\n",
    "X = new_df.drop('default_payment',axis=1)\n",
    "#scale all our data using robust scaler\n",
    "robust_scaler = RobustScaler()\n",
    "X = robust_scaler.fit_transform(X)\n",
    "#y is the label of our data\n",
    "y = new_df['default_payment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a train test split of the data with test size 30%\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=123, stratify=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our dataset has class imbalance which needs to be taken cared to build a better model which would not over/underfit when learning for classification. Let us handle class imbalance before we start building different model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Taking care of Class Imbalance\n",
    "Balancing the class weight\n",
    "\n",
    "Modify the dataset (resampling)\n",
    "\n",
    "Ensemble methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a dataframe to hold the recall score from different resampling techniques\n",
    "resampled_score = pd.DataFrame(columns=['method','recall','precision','f1_score','AUC'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate a logistic regression classifier: logreg\n",
    "model = LogisticRegression(solver='lbfgs')\n",
    "\n",
    "# Setup the hyperparameter grid\n",
    "param_dist = {'C': [0.001,0.01,0.1,1,10,100,1000]}\n",
    "\n",
    "# Instantiate the RandomizedSearchCV object: logreg_cv\n",
    "model_cv = RandomizedSearchCV(model, param_distributions=param_dist, cv=5, n_iter=4, random_state=0)\n",
    "\n",
    "start = time()\n",
    "\n",
    "# Fit it to the data\n",
    "model_cv.fit(X_train,y_train)\n",
    "\n",
    "print('RandomizedSearchCV took %.2f seconds for %d candidates'\n",
    "      ' parameter settings.' % ((time() - start), model_cv.n_iter))\n",
    "\n",
    "y_pred_test = model_cv.predict(X_test)\n",
    "\n",
    "#to calculate the auc score\n",
    "false_positive_rate, true_positive_rate, thresholds = roc_curve(y_test, y_pred_test)\n",
    "roc_auc = auc(false_positive_rate, true_positive_rate)\n",
    "#calculate other metrics scores\n",
    "accuracy = accuracy_score(y_pred=y_pred_test, y_true=y_test)\n",
    "precision = precision_score(y_pred=y_pred_test, y_true=y_test)\n",
    "recall = recall_score(y_pred=y_pred_test,y_true=y_test)\n",
    "f1score = f1_score(y_true=y_test, y_pred=y_pred_test)\n",
    "\n",
    "resampled_score = resampled_score.append({'method': 'not resampled','recall':recall,'precision':precision, 'f1_score':f1score, 'AUC':roc_auc}, ignore_index=True)\n",
    "\n",
    "# Print the tuned parameters and score\n",
    "print('Tuned Logistic Regression Parameters: {}'.format(model_cv.best_params_))\n",
    "print('Best score is {0:.4f}'.format(model_cv.best_score_))\n",
    "print('AUC Score: {0:.4f}'.format(roc_auc))\n",
    "print('Accuracy Score: {0:.4f}'.format(accuracy))\n",
    "print('Precision Score: {0:.4f}'.format(precision))\n",
    "print('Recall Score: {0:.4f}'.format(recall))\n",
    "print('f1 score: {0:.4f}'.format(f1score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test, y_pred_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Balancing the class weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate a logistic regression classifier: logreg\n",
    "model = LogisticRegression(class_weight='balanced', solver='lbfgs')\n",
    "\n",
    "# Setup the hyperparameter grid\n",
    "param_dist = {'C': [0.001,0.01,0.1,1,10,100,1000]}\n",
    "\n",
    "# Instantiate the RandomizedSearchCV object: logreg_cv\n",
    "balanced_model_cv = RandomizedSearchCV(model, param_distributions=param_dist, cv=5, n_iter=4, random_state=0)\n",
    "\n",
    "start = time()\n",
    "\n",
    "# Fit it to the data\n",
    "balanced_model_cv.fit(X_train,y_train)\n",
    "\n",
    "print('RandomizedSearchCV took %.2f seconds for %d candidates'\n",
    "      ' parameter settings.' % ((time() - start), model_cv.n_iter))\n",
    "\n",
    "y_pred_test = balanced_model_cv.predict(X_test)\n",
    "\n",
    "#to calculate the auc score\n",
    "false_positive_rate, true_positive_rate, thresholds = roc_curve(y_test, y_pred_test)\n",
    "roc_auc = auc(false_positive_rate, true_positive_rate)\n",
    "#calculate other metrics scores\n",
    "accuracy = accuracy_score(y_pred=y_pred_test, y_true=y_test)\n",
    "precision = precision_score(y_pred=y_pred_test, y_true=y_test)\n",
    "recall = recall_score(y_pred=y_pred_test,y_true=y_test)\n",
    "f1score = f1_score(y_true=y_test, y_pred=y_pred_test)\n",
    "\n",
    "resampled_score = resampled_score.append({'method': 'Balancing class weight','recall':recall,'precision':precision, 'f1_score':f1score, 'AUC':roc_auc}, ignore_index=True)\n",
    "\n",
    "\n",
    "# Print the tuned parameters and score\n",
    "print('Tuned Logistic Regression with Class weight balanced Parameters: {}'.format(balanced_model_cv.best_params_))\n",
    "print('Best score is {0:.4f}'.format(balanced_model_cv.best_score_))\n",
    "print('AUC Score: {0:.4f}'.format(roc_auc))\n",
    "print('Accuracy Score: {0:.4f}'.format(accuracy))\n",
    "print('Precision Score: {0:.4f}'.format(precision))\n",
    "print('Recall Score: {0:.4f}'.format(recall))\n",
    "print('f1 score: {0:.4f}'.format(f1score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test, y_pred_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now Undersampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Random Undersampling\n",
    "us = RandomUnderSampler(ratio=0.5, random_state=1)\n",
    "X_train_res, y_train_res = us.fit_sample(X_train, y_train)\n",
    "\n",
    "print('Distribution of class labels before resampling {}'.format(Counter(y_train)))\n",
    "print('Distribution of class labels before resampling {}'.format(Counter(y_train_res)))\n",
    "\n",
    "clf_base = LogisticRegression(solver='lbfgs')\n",
    "grid = {'C': 10.0**np.arange(-2,3)}\n",
    "\n",
    "#cv = KFold(n_splits=5, shuffle=True, random_state=0)\n",
    "clf = GridSearchCV(clf_base, grid, cv=5, scoring='f1')\n",
    "\n",
    "clf.fit(X_train_res, y_train_res)\n",
    "\n",
    "y_pred_test = clf.predict(X_test)\n",
    "#to calculate the auc score\n",
    "false_positive_rate, true_positive_rate, thresholds = roc_curve(y_test, y_pred_test)\n",
    "roc_auc = auc(false_positive_rate, true_positive_rate)\n",
    "#calculate other metrics scores\n",
    "accuracy = accuracy_score(y_pred=y_pred_test, y_true=y_test)\n",
    "precision = precision_score(y_pred=y_pred_test, y_true=y_test)\n",
    "recall = recall_score(y_pred=y_pred_test,y_true=y_test)\n",
    "f1score = f1_score(y_true=y_test, y_pred=y_pred_test)\n",
    "\n",
    "resampled_score = resampled_score.append({'method': 'RandomUnderSampler','recall':recall, 'precision':precision, 'f1_score':f1score, 'AUC':roc_auc}, ignore_index=True)\n",
    "\n",
    "# Print the tuned parameters and score\n",
    "print('Tuned Logistic Regression with RandomUnderSampler Parameters: {}'.format(clf.best_params_))\n",
    "print('Best score is {0:.4f}'.format(clf.best_score_))\n",
    "print('AUC Score: {0:.4f}'.format(roc_auc))\n",
    "print('Accuracy Score: {0:.4f}'.format(accuracy))\n",
    "print('Precision Score: {0:.4f}'.format(precision))\n",
    "print('Recall Score: {0:.4f}'.format(recall))\n",
    "print('f1 score: {0:.4f}'.format(f1score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NEARMISS-1\n",
    "us = NearMiss(ratio=0.5, version=1, random_state=1)\n",
    "X_train_res, y_train_res = us.fit_sample(X_train, y_train)\n",
    "\n",
    "print('Distribution of class labels before resampling {}'.format(Counter(y_train)))\n",
    "print('Distribution of class labels after resampling {}'.format(Counter(y_train_res)))\n",
    "\n",
    "clf_base = LogisticRegression(solver='lbfgs')\n",
    "grid = {'C': 10.0 ** np.arange(-2,3)}\n",
    "\n",
    "clf = GridSearchCV(clf_base, grid, cv=5, scoring='f1')\n",
    "\n",
    "clf.fit(X_train_res, y_train_res)\n",
    "\n",
    "y_pred_test = clf.predict(X_test)\n",
    "\n",
    "#to calculate the auc score\n",
    "false_positive_rate, true_positive_rate, thresholds = roc_curve(y_test, y_pred_test)\n",
    "roc_auc = auc(false_positive_rate, true_positive_rate)\n",
    "#calculate other metrics scores\n",
    "accuracy = accuracy_score(y_pred=y_pred_test, y_true=y_test)\n",
    "precision = precision_score(y_pred=y_pred_test, y_true=y_test)\n",
    "recall = recall_score(y_pred=y_pred_test,y_true=y_test)\n",
    "f1score = f1_score(y_true=y_test, y_pred=y_pred_test)\n",
    "\n",
    "#add the recall score to the dataframe resampled_score\n",
    "resampled_score = resampled_score.append({'method': 'NearMiss1','recall':recall, 'precision':precision, 'f1_score':f1score, 'AUC':roc_auc}, ignore_index=True)\n",
    "\n",
    "# Print the tuned parameters and score\n",
    "print('Tuned Logistic Regression with NearMiss1 Parameters: {}'.format(clf.best_params_))\n",
    "print('Best score is {0:.4f}'.format(clf.best_score_))\n",
    "print('AUC Score: {0:.4f}'.format(roc_auc))\n",
    "print('Accuracy Score: {0:.4f}'.format(accuracy))\n",
    "print('Precision Score: {0:.4f}'.format(precision))\n",
    "print('Recall Score: {0:.4f}'.format(recall))\n",
    "print('f1 score: {0:.4f}'.format(f1score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NEARMISS-2\n",
    "us = NearMiss(ratio=0.5, version=2, random_state=1)\n",
    "X_train_res, y_train_res = us.fit_sample(X_train, y_train)\n",
    "\n",
    "print('Distribution of class labels before resampling {}'.format(Counter(y_train)))\n",
    "print('Distribution of class labels after resampling {}'.format(Counter(y_train_res)))\n",
    "\n",
    "clf_base = LogisticRegression(solver='lbfgs')\n",
    "grid = {'C': 10.0 ** np.arange(-2,3)}\n",
    "\n",
    "clf = GridSearchCV(clf_base, grid, cv=5, scoring='f1')\n",
    "\n",
    "clf.fit(X_train_res, y_train_res)\n",
    "\n",
    "y_pred_test = clf.predict(X_test)\n",
    "\n",
    "#to calculate the auc score\n",
    "false_positive_rate, true_positive_rate, thresholds = roc_curve(y_test, y_pred_test)\n",
    "roc_auc = auc(false_positive_rate, true_positive_rate)\n",
    "#calculate other metrics scores\n",
    "accuracy = accuracy_score(y_pred=y_pred_test, y_true=y_test)\n",
    "precision = precision_score(y_pred=y_pred_test, y_true=y_test)\n",
    "recall = recall_score(y_pred=y_pred_test,y_true=y_test)\n",
    "f1score = f1_score(y_true=y_test, y_pred=y_pred_test)\n",
    "\n",
    "#add the recall score to the dataframe resampled_score\n",
    "resampled_score = resampled_score.append({'method': 'NearMiss2','recall':recall, 'precision':precision,'f1_score':f1score, 'AUC':roc_auc}, ignore_index=True)\n",
    "\n",
    "# Print the tuned parameters and score\n",
    "print('Tuned Logistic Regression with NearMiss2 Parameters: {}'.format(clf.best_params_))\n",
    "print('Best score is {0:.4f}'.format(clf.best_score_))\n",
    "print('AUC Score: {0:.4f}'.format(roc_auc))\n",
    "print('Accuracy Score: {0:.4f}'.format(accuracy))\n",
    "print('Precision Score: {0:.4f}'.format(precision))\n",
    "print('Recall Score: {0:.4f}'.format(recall))\n",
    "print('f1 score: {0:.4f}'.format(f1score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Edited Nearest Neighbour\n",
    "us = EditedNearestNeighbours(random_state=0)\n",
    "X_train_res, y_train_res = us.fit_sample(X_train, y_train)\n",
    "\n",
    "print('Distribution of class labels before resampling {}'.format(Counter(y_train)))\n",
    "print('Distribution of class labels after resampling {}'.format(Counter(y_train_res)))\n",
    "\n",
    "clf_base = LogisticRegression(solver='lbfgs')\n",
    "grid = {'C': 10.0 ** np.arange(-2,3)}\n",
    "\n",
    "clf = GridSearchCV(clf_base, grid, cv=5, scoring='f1')\n",
    "\n",
    "clf.fit(X_train_res, y_train_res)\n",
    "\n",
    "y_pred_test = clf.predict(X_test)\n",
    "\n",
    "#to calculate the auc score\n",
    "false_positive_rate, true_positive_rate, thresholds = roc_curve(y_test, y_pred_test)\n",
    "roc_auc = auc(false_positive_rate, true_positive_rate)\n",
    "#calculate other metrics scores\n",
    "accuracy = accuracy_score(y_pred=y_pred_test, y_true=y_test)\n",
    "precision = precision_score(y_pred=y_pred_test, y_true=y_test)\n",
    "recall = recall_score(y_pred=y_pred_test,y_true=y_test)\n",
    "f1score = f1_score(y_true=y_test, y_pred=y_pred_test)\n",
    "\n",
    "#add the recall score to the dataframe resampled_score\n",
    "resampled_score = resampled_score.append({'method': 'EditedNearestNeighbour','recall':recall, 'precision':precision, 'f1_score':f1score, 'AUC':roc_auc}, ignore_index=True)\n",
    "\n",
    "# Print the tuned parameters and score\n",
    "print('Tuned Logistic Regression with EditedNearestNeighbours Parameters: {}'.format(clf.best_params_))\n",
    "print('Best score is {0:.4f}'.format(clf.best_score_))\n",
    "print('AUC Score: {0:.4f}'.format(roc_auc))\n",
    "print('Accuracy Score: {0:.4f}'.format(accuracy))\n",
    "print('Precision Score: {0:.4f}'.format(precision))\n",
    "print('Recall Score: {0:.4f}'.format(recall))\n",
    "print('f1 score: {0:.4f}'.format(f1score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Repeated Edited Nearest Neighbour\n",
    "us = RepeatedEditedNearestNeighbours(random_state=0)\n",
    "X_train_res, y_train_res = us.fit_sample(X_train, y_train)\n",
    "\n",
    "print('Distribution of class labels before resampling {}'.format(Counter(y_train)))\n",
    "print('Distribution of class labels after resampling {}'.format(Counter(y_train_res)))\n",
    "\n",
    "clf_base = LogisticRegression(solver='lbfgs')\n",
    "grid = {'C': 10.0 ** np.arange(-2,3)}\n",
    "\n",
    "clf = GridSearchCV(clf_base, grid, cv=5, scoring='f1')\n",
    "\n",
    "clf.fit(X_train_res, y_train_res)\n",
    "\n",
    "y_pred_test = clf.predict(X_test)\n",
    "\n",
    "#to calculate the auc score\n",
    "false_positive_rate, true_positive_rate, thresholds = roc_curve(y_test, y_pred_test)\n",
    "roc_auc = auc(false_positive_rate, true_positive_rate)\n",
    "#calculate other metrics scores\n",
    "accuracy = accuracy_score(y_pred=y_pred_test, y_true=y_test)\n",
    "precision = precision_score(y_pred=y_pred_test, y_true=y_test)\n",
    "recall = recall_score(y_pred=y_pred_test,y_true=y_test)\n",
    "f1score = f1_score(y_true=y_test, y_pred=y_pred_test)\n",
    "\n",
    "#add the recall score to the dataframe resampled_score\n",
    "resampled_score = resampled_score.append({'method': 'RepeatedEditedNearestNeighbours','recall':recall,'precision':precision,'f1_score':f1score, 'AUC':roc_auc}, ignore_index=True)\n",
    "\n",
    "# Print the tuned parameters and score\n",
    "print('Tuned Logistic Regression with RepeatedEditedNearestNeighbours Parameters: {}'.format(clf.best_params_))\n",
    "print('Best score is {0:.4f}'.format(clf.best_score_))\n",
    "print('AUC Score: {0:.4f}'.format(roc_auc))\n",
    "print('Accuracy Score: {0:.4f}'.format(accuracy))\n",
    "print('Precision Score: {0:.4f}'.format(precision))\n",
    "print('Recall Score: {0:.4f}'.format(recall))\n",
    "print('f1 score: {0:.4f}'.format(f1score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tomek Link Removal\n",
    "us = TomekLinks(random_state=0)\n",
    "X_train_res, y_train_res = us.fit_sample(X_train, y_train)\n",
    "\n",
    "print('Distribution of class labels before resampling {}'.format(Counter(y_train)))\n",
    "print('Distribution of class labels after resampling {}'.format(Counter(y_train_res)))\n",
    "\n",
    "clf_base = LogisticRegression(solver='lbfgs')\n",
    "grid = {'C': 10.0 ** np.arange(-2,3)}\n",
    "\n",
    "clf = GridSearchCV(clf_base, grid, cv=5, scoring='f1')\n",
    "\n",
    "clf.fit(X_train_res, y_train_res)\n",
    "\n",
    "y_pred_test = clf.predict(X_test)\n",
    "\n",
    "#to calculate the auc score\n",
    "false_positive_rate, true_positive_rate, thresholds = roc_curve(y_test, y_pred_test)\n",
    "roc_auc = auc(false_positive_rate, true_positive_rate)\n",
    "#calculate other metrics scores\n",
    "accuracy = accuracy_score(y_pred=y_pred_test, y_true=y_test)\n",
    "precision = precision_score(y_pred=y_pred_test, y_true=y_test)\n",
    "recall = recall_score(y_pred=y_pred_test,y_true=y_test)\n",
    "f1score = f1_score(y_true=y_test, y_pred=y_pred_test)\n",
    "\n",
    "#add the recall score to the dataframe resampled_score\n",
    "resampled_score = resampled_score.append({'method': 'TomekLinks','recall':recall, 'precision':precision, 'f1_score':f1score, 'AUC':roc_auc}, ignore_index=True)\n",
    "\n",
    "# Print the tuned parameters and score\n",
    "print('Tuned Logistic Regression with TomekLinks: {}'.format(clf.best_params_))\n",
    "print('Best score is {0:.4f}'.format(clf.best_score_))\n",
    "print('AUC Score: {0:.4f}'.format(roc_auc))\n",
    "print('Accuracy Score: {0:.4f}'.format(accuracy))\n",
    "print('Precision Score: {0:.4f}'.format(precision))\n",
    "print('Recall Score: {0:.4f}'.format(recall))\n",
    "print('f1 score: {0:.4f}'.format(f1score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Oversampling methods\n",
    "\n",
    "Number of minority class data will increase by copying the data we already have."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Random Oversampling\n",
    "os = RandomOverSampler(ratio=0.5,random_state=0)\n",
    "X_train_res, y_train_res = os.fit_sample(X_train, y_train)\n",
    "\n",
    "print('Distribution of class labels before resampling {}'.format(Counter(y_train)))\n",
    "print('Distribution of class labels after resampling {}'.format(Counter(y_train_res)))\n",
    "\n",
    "clf_base = LogisticRegression(solver='lbfgs')\n",
    "grid = {'C': 10.0 ** np.arange(-2,3)}\n",
    "\n",
    "clf = GridSearchCV(clf_base, grid, cv=5, scoring='f1')\n",
    "\n",
    "clf.fit(X_train_res, y_train_res)\n",
    "\n",
    "y_pred_test = clf.predict(X_test)\n",
    "\n",
    "#to calculate the auc score\n",
    "false_positive_rate, true_positive_rate, thresholds = roc_curve(y_test, y_pred_test)\n",
    "roc_auc = auc(false_positive_rate, true_positive_rate)\n",
    "#calculate other metrics scores\n",
    "accuracy = accuracy_score(y_pred=y_pred_test, y_true=y_test)\n",
    "precision = precision_score(y_pred=y_pred_test, y_true=y_test)\n",
    "recall = recall_score(y_pred=y_pred_test,y_true=y_test)\n",
    "f1score = f1_score(y_true=y_test, y_pred=y_pred_test)\n",
    "\n",
    "#add the recall score to the dataframe resampled_score\n",
    "resampled_score = resampled_score.append({'method': 'RandomOverSampler','recall':recall, 'precision':precision, 'f1_score':f1score,'AUC':roc_auc}, ignore_index=True)\n",
    "\n",
    "# Print the tuned parameters and score\n",
    "print('Tuned Logistic Regression with RandomOverSampler Parameters: {}'.format(clf.best_params_))\n",
    "print('Best score is {0:.4f}'.format(clf.best_score_))\n",
    "print('AUC Score: {0:.4f}'.format(roc_auc))\n",
    "print('Accuracy Score: {0:.4f}'.format(accuracy))\n",
    "print('Precision Score: {0:.4f}'.format(precision))\n",
    "print('Recall Score: {0:.4f}'.format(recall))\n",
    "print('f1 score: {0:.4f}'.format(f1score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SMOTE\n",
    "os = SMOTE(ratio=0.5, random_state=0)\n",
    "X_train_res, y_train_res = os.fit_sample(X_train, y_train)\n",
    "\n",
    "print('Distribution of class labels before resampling {}'.format(Counter(y_train)))\n",
    "print('Distribution of class labels after resampling {}'.format(Counter(y_train_res)))\n",
    "\n",
    "clf_base = LogisticRegression(solver='lbfgs')\n",
    "grid = {'C': 10.0 ** np.arange(-2,3)}\n",
    "\n",
    "clf = GridSearchCV(clf_base, grid, cv=5, scoring='f1')\n",
    "\n",
    "clf.fit(X_train_res, y_train_res)\n",
    "\n",
    "y_pred_test = clf.predict(X_test)\n",
    "\n",
    "#to calculate the auc score\n",
    "false_positive_rate, true_positive_rate, thresholds = roc_curve(y_test, y_pred_test)\n",
    "roc_auc = auc(false_positive_rate, true_positive_rate)\n",
    "#calculate other metrics scores\n",
    "accuracy = accuracy_score(y_pred=y_pred_test, y_true=y_test)\n",
    "precision = precision_score(y_pred=y_pred_test, y_true=y_test)\n",
    "recall = recall_score(y_pred=y_pred_test,y_true=y_test)\n",
    "f1score = f1_score(y_true=y_test, y_pred=y_pred_test)\n",
    "\n",
    "#add the recall score to the dataframe resampled_score\n",
    "resampled_score = resampled_score.append({'method': 'SMOTE','recall':recall, 'precision':precision, 'f1_score':f1score, 'AUC':roc_auc}, ignore_index=True)\n",
    "\n",
    "# Print the tuned parameters and score\n",
    "print('Tuned Logistic Regression with SMOTE Parameters: {}'.format(clf.best_params_))\n",
    "print('Best score is {0:.4f}'.format(clf.best_score_))\n",
    "print('AUC Score: {0:.4f}'.format(roc_auc))\n",
    "print('Accuracy Score: {0:.4f}'.format(accuracy))\n",
    "print('Precision Score: {0:.4f}'.format(precision))\n",
    "print('Recall Score: {0:.4f}'.format(recall))\n",
    "print('f1 score: {0:.4f}'.format(f1score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ensemble methods for handling class imabalance is combining two methods of handling class imbalance. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combinations\n",
    "SMOTE + Tomek Link Removal\n",
    "\n",
    "SMOTE + ENN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SMOTE + Tomek link removal\n",
    "os_us = SMOTETomek(ratio=0.5, random_state=0)\n",
    "X_train_res, y_train_res = os_us.fit_sample(X_train, y_train)\n",
    "\n",
    "print('Distribution of class labels before resampling {}'.format(Counter(y_train)))\n",
    "print('Distribution of class labels after resampling {}'.format(Counter(y_train_res)))\n",
    "\n",
    "clf_base = LogisticRegression(solver='lbfgs')\n",
    "grid = {'C': 10.0 ** np.arange(-2,3)}\n",
    "\n",
    "clf = GridSearchCV(clf_base, grid, cv=5, scoring='f1')\n",
    "\n",
    "clf.fit(X_train_res, y_train_res)\n",
    "\n",
    "y_pred_test = clf.predict(X_test)\n",
    "\n",
    "#to calculate the auc score\n",
    "false_positive_rate, true_positive_rate, thresholds = roc_curve(y_test, y_pred_test)\n",
    "roc_auc = auc(false_positive_rate, true_positive_rate)\n",
    "#calculate other metrics scores\n",
    "accuracy = accuracy_score(y_pred=y_pred_test, y_true=y_test)\n",
    "precision = precision_score(y_pred=y_pred_test, y_true=y_test)\n",
    "recall = recall_score(y_pred=y_pred_test,y_true=y_test)\n",
    "f1score = f1_score(y_true=y_test, y_pred=y_pred_test)\n",
    "\n",
    "#add the recall score to the dataframe resampled_score\n",
    "resampled_score = resampled_score.append({'method': 'SMOTETomek','recall':recall, 'precision':precision, 'f1_score':f1score, 'AUC':roc_auc}, ignore_index=True)\n",
    "\n",
    "# Print the tuned parameters and score\n",
    "print('Tuned Logistic Regression with SMOTETomek Parameters: {}'.format(clf.best_params_))\n",
    "print('Best score is {0:.4f}'.format(clf.best_score_))\n",
    "print('AUC Score: {0:.4f}'.format(roc_auc))\n",
    "print('Accuracy Score: {0:.4f}'.format(accuracy))\n",
    "print('Precision Score: {0:.4f}'.format(precision))\n",
    "print('Recall Score: {0:.4f}'.format(recall))\n",
    "print('f1 score: {0:.4f}'.format(f1score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SMOTE + ENN\n",
    "os_us = SMOTEENN(ratio=0.5, random_state=0)\n",
    "X_train_res, y_train_res = os_us.fit_sample(X_train, y_train)\n",
    "\n",
    "print('Distribution of class labels before resampling {}'.format(Counter(y_train)))\n",
    "print('Distribution of class labels after resampling {}'.format(Counter(y_train_res)))\n",
    "\n",
    "clf_base = LogisticRegression(solver='lbfgs')\n",
    "grid = {'C': 10.0 ** np.arange(-2,3)}\n",
    "\n",
    "clf = GridSearchCV(clf_base, grid, cv=5, scoring='f1')\n",
    "\n",
    "clf.fit(X_train_res, y_train_res)\n",
    "\n",
    "y_pred_test = clf.predict(X_test)\n",
    "\n",
    "#to calculate the auc score\n",
    "false_positive_rate, true_positive_rate, thresholds = roc_curve(y_test, y_pred_test)\n",
    "roc_auc = auc(false_positive_rate, true_positive_rate)\n",
    "#calculate other metrics scores\n",
    "accuracy = accuracy_score(y_pred=y_pred_test, y_true=y_test)\n",
    "precision = precision_score(y_pred=y_pred_test, y_true=y_test)\n",
    "recall = recall_score(y_pred=y_pred_test,y_true=y_test)\n",
    "f1score = f1_score(y_true=y_test, y_pred=y_pred_test)\n",
    "\n",
    "#add the recall score to the dataframe resampled_score\n",
    "resampled_score = resampled_score.append({'method': 'SMOTEENN','recall':recall, 'precision':precision, 'f1_score':f1score, 'AUC':roc_auc}, ignore_index=True)\n",
    "\n",
    "# Print the tuned parameters and score\n",
    "print('Tuned Logistic Regression with SMOTEENN Parameters: {}'.format(clf.best_params_))\n",
    "print('Best score is {0:.4f}'.format(clf.best_score_))\n",
    "print('AUC Score: {0:.4f}'.format(roc_auc))\n",
    "print('Accuracy Score: {0:.4f}'.format(accuracy))\n",
    "print('Precision Score: {0:.4f}'.format(precision))\n",
    "print('Recall Score: {0:.4f}'.format(recall))\n",
    "print('f1 score: {0:.4f}'.format(f1score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resampled_score.sort_values(by='f1_score', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to the F1 Score of the classification model, the SMOTE+ENN method to handle the class imbalance performed the best. So we will use SMOTE+ENN to handle class imbalance, then compare different models with each other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Preparing a dataframe for model analysis\n",
    "#Data frame for evaluation metrics\n",
    "smoteenn_resampled_metrics = pd.DataFrame(index=['roc_auc', 'accuracy','precision','recall','f1'],columns=['LogisticReg','DecisionTree','GaussianNB','RandomForest','SVM'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SMOTE+ENN with Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SMOTE + ENN\n",
    "os_us = SMOTEENN(ratio=0.5, random_state=0)\n",
    "X_train_res, y_train_res = os_us.fit_sample(X_train, y_train)\n",
    "\n",
    "print('Distribution of class labels before resampling {}'.format(Counter(y_train)))\n",
    "print('Distribution of class labels after resampling {}'.format(Counter(y_train_res)))\n",
    "\n",
    "clf = LogisticRegression(solver='lbfgs')\n",
    "\n",
    "clf.fit(X_train_res, y_train_res)\n",
    "\n",
    "y_pred_test = clf.predict(X_test)\n",
    "\n",
    "#to calculate the auc score\n",
    "false_positive_rate, true_positive_rate, thresholds = roc_curve(y_test, y_pred_test)\n",
    "roc_auc = auc(false_positive_rate, true_positive_rate)\n",
    "#calculate other metrics scores\n",
    "accuracy = accuracy_score(y_pred=y_pred_test, y_true=y_test)\n",
    "precision = precision_score(y_pred=y_pred_test, y_true=y_test)\n",
    "recall = recall_score(y_pred=y_pred_test,y_true=y_test)\n",
    "f1score = f1_score(y_true=y_test, y_pred=y_pred_test)\n",
    "\n",
    "#entering tuned metrics score to the dataframe\n",
    "smoteenn_resampled_metrics.loc['accuracy','LogisticReg'] = accuracy\n",
    "smoteenn_resampled_metrics.loc['precision','LogisticReg'] = precision\n",
    "smoteenn_resampled_metrics.loc['recall','LogisticReg'] = recall\n",
    "smoteenn_resampled_metrics.loc['roc_auc','LogisticReg'] = roc_auc\n",
    "smoteenn_resampled_metrics.loc['f1','LogisticReg'] = f1score\n",
    "\n",
    "# Print different metrics score\n",
    "print('AUC Score: {0:.4f}'.format(roc_auc))\n",
    "print('Accuracy Score: {0:.4f}'.format(accuracy))\n",
    "print('Precision Score: {0:.4f}'.format(precision))\n",
    "print('Recall Score: {0:.4f}'.format(recall))\n",
    "print('f1 score: {0:.4f}'.format(f1score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SMOTE+ENN with DecisionTree Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SMOTE + ENN\n",
    "clf = DecisionTreeClassifier()\n",
    "\n",
    "clf.fit(X_train_res, y_train_res)\n",
    "\n",
    "y_pred_test = clf.predict(X_test)\n",
    "\n",
    "#to calculate the auc score\n",
    "false_positive_rate, true_positive_rate, thresholds = roc_curve(y_test, y_pred_test)\n",
    "roc_auc = auc(false_positive_rate, true_positive_rate)\n",
    "#calculate other metrics scores\n",
    "accuracy = accuracy_score(y_pred=y_pred_test, y_true=y_test)\n",
    "precision = precision_score(y_pred=y_pred_test, y_true=y_test)\n",
    "recall = recall_score(y_pred=y_pred_test,y_true=y_test)\n",
    "f1score = f1_score(y_true=y_test, y_pred=y_pred_test)\n",
    "\n",
    "#entering tuned metrics score to the dataframe\n",
    "smoteenn_resampled_metrics.loc['accuracy','DecisionTree'] = accuracy\n",
    "smoteenn_resampled_metrics.loc['precision','DecisionTree'] = precision\n",
    "smoteenn_resampled_metrics.loc['recall','DecisionTree'] = recall\n",
    "smoteenn_resampled_metrics.loc['roc_auc','DecisionTree'] = roc_auc\n",
    "smoteenn_resampled_metrics.loc['f1','DecisionTree'] = f1score\n",
    "\n",
    "# Print different metrics score\n",
    "print('AUC Score: {0:.4f}'.format(roc_auc))\n",
    "print('Accuracy Score: {0:.4f}'.format(accuracy))\n",
    "print('Precision Score: {0:.4f}'.format(precision))\n",
    "print('Recall Score: {0:.4f}'.format(recall))\n",
    "print('f1 score: {0:.4f}'.format(f1score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SMOTE+ENN with GaussianNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SMOTE + ENN\n",
    "clf = GaussianNB()\n",
    "\n",
    "clf.fit(X_train_res, y_train_res)\n",
    "\n",
    "y_pred_test = clf.predict(X_test)\n",
    "\n",
    "#to calculate the auc score\n",
    "false_positive_rate, true_positive_rate, thresholds = roc_curve(y_test, y_pred_test)\n",
    "roc_auc = auc(false_positive_rate, true_positive_rate)\n",
    "#calculate other metrics scores\n",
    "accuracy = accuracy_score(y_pred=y_pred_test, y_true=y_test)\n",
    "precision = precision_score(y_pred=y_pred_test, y_true=y_test)\n",
    "recall = recall_score(y_pred=y_pred_test,y_true=y_test)\n",
    "f1score = f1_score(y_true=y_test, y_pred=y_pred_test)\n",
    "\n",
    "#entering tuned metrics score to the dataframe\n",
    "smoteenn_resampled_metrics.loc['accuracy','GaussianNB'] = accuracy\n",
    "smoteenn_resampled_metrics.loc['precision','GaussianNB'] = precision\n",
    "smoteenn_resampled_metrics.loc['recall','GaussianNB'] = recall\n",
    "smoteenn_resampled_metrics.loc['roc_auc','GaussianNB'] = roc_auc\n",
    "smoteenn_resampled_metrics.loc['f1','GaussianNB'] = f1score\n",
    "\n",
    "# Print the different metrics score\n",
    "print('AUC Score: {0:.4f}'.format(roc_auc))\n",
    "print('Accuracy Score: {0:.4f}'.format(accuracy))\n",
    "print('Precision Score: {0:.4f}'.format(precision))\n",
    "print('Recall Score: {0:.4f}'.format(recall))\n",
    "print('f1 score: {0:.4f}'.format(f1score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SMOTE+ENN with RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SMOTE + ENN\n",
    "clf = RandomForestClassifier()\n",
    "\n",
    "clf.fit(X_train_res, y_train_res)\n",
    "\n",
    "y_pred_test = clf.predict(X_test)\n",
    "\n",
    "#to calculate the auc score\n",
    "false_positive_rate, true_positive_rate, thresholds = roc_curve(y_test, y_pred_test)\n",
    "roc_auc = auc(false_positive_rate, true_positive_rate)\n",
    "#calculate other metrics scores\n",
    "accuracy = accuracy_score(y_pred=y_pred_test, y_true=y_test)\n",
    "precision = precision_score(y_pred=y_pred_test, y_true=y_test)\n",
    "recall = recall_score(y_pred=y_pred_test,y_true=y_test)\n",
    "f1score = f1_score(y_true=y_test, y_pred=y_pred_test)\n",
    "\n",
    "#entering tuned metrics score to the dataframe\n",
    "smoteenn_resampled_metrics.loc['accuracy','RandomForest'] = accuracy\n",
    "smoteenn_resampled_metrics.loc['precision','RandomForest'] = precision\n",
    "smoteenn_resampled_metrics.loc['recall','RandomForest'] = recall\n",
    "smoteenn_resampled_metrics.loc['roc_auc','RandomForest'] = roc_auc\n",
    "smoteenn_resampled_metrics.loc['f1','RandomForest'] = f1score\n",
    "\n",
    "# Print different metrics score\n",
    "print('AUC Score: {0:.4f}'.format(roc_auc))\n",
    "print('Accuracy Score: {0:.4f}'.format(accuracy))\n",
    "print('Precision Score: {0:.4f}'.format(precision))\n",
    "print('Recall Score: {0:.4f}'.format(recall))\n",
    "print('f1 score: {0:.4f}'.format(f1score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SMOTE+ENN with SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SMOTE + ENN\n",
    "clf = svm.SVC(kernel='linear')\n",
    "\n",
    "clf.fit(X_train_res, y_train_res)\n",
    "\n",
    "y_pred_test = clf.predict(X_test)\n",
    "\n",
    "#to calculate the auc score\n",
    "false_positive_rate, true_positive_rate, thresholds = roc_curve(y_test, y_pred_test)\n",
    "roc_auc = auc(false_positive_rate, true_positive_rate)\n",
    "#calculate other metrics scores\n",
    "accuracy = accuracy_score(y_pred=y_pred_test, y_true=y_test)\n",
    "precision = precision_score(y_pred=y_pred_test, y_true=y_test)\n",
    "recall = recall_score(y_pred=y_pred_test,y_true=y_test)\n",
    "f1score = f1_score(y_true=y_test, y_pred=y_pred_test)\n",
    "\n",
    "#entering tuned metrics score to the dataframe\n",
    "smoteenn_resampled_metrics.loc['accuracy','SVM'] = accuracy\n",
    "smoteenn_resampled_metrics.loc['precision','SVM'] = precision\n",
    "smoteenn_resampled_metrics.loc['recall','SVM'] = recall\n",
    "smoteenn_resampled_metrics.loc['roc_auc','SVM'] = roc_auc\n",
    "smoteenn_resampled_metrics.loc['f1','SVM'] = f1score\n",
    "\n",
    "# Print different metrics score\n",
    "print('AUC Score: {0:.4f}'.format(roc_auc))\n",
    "print('Accuracy Score: {0:.4f}'.format(accuracy))\n",
    "print('Precision Score: {0:.4f}'.format(precision))\n",
    "print('Recall Score: {0:.4f}'.format(recall))\n",
    "print('f1 score: {0:.4f}'.format(f1score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "100*smoteenn_resampled_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic Regression has the best f1 score compared to all other classification model, so I will choose Logistic Regression for this project. Since the F1-score is significantly better than a random classifier, I would recommend this Logistic Regression model to predict credit card default. Let's try to tune all our model to see if we can improve the performance and then decide which classification model would be the best. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter Tuning for model performance improvement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Preparing a dataframe for model analysis\n",
    "#Data frame for evaluation metrics\n",
    "smoteenn_resampled_tuned_metrics = pd.DataFrame(index=['roc_auc', 'accuracy','precision','recall','f1'],columns=['LogisticReg','DecisionTree','GaussianNB','RandomForest','SVM'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SMOTE+ENN with Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SMOTE + ENN\n",
    "os_us = SMOTEENN(ratio=0.5, random_state=0)\n",
    "X_train_res, y_train_res = os_us.fit_sample(X_train, y_train)\n",
    "\n",
    "print('Distribution of class labels before resampling {}'.format(Counter(y_train)))\n",
    "print('Distribution of class labels after resampling {}'.format(Counter(y_train_res)))\n",
    "\n",
    "clf_base = LogisticRegression(solver='lbfgs')\n",
    "grid = {'C': 10.0 ** np.arange(-2,3)}\n",
    "\n",
    "clf = GridSearchCV(clf_base, grid, cv=5, scoring='f1')\n",
    "\n",
    "clf.fit(X_train_res, y_train_res)\n",
    "\n",
    "y_pred_test = clf.predict(X_test)\n",
    "\n",
    "#to calculate the auc score\n",
    "false_positive_rate, true_positive_rate, thresholds = roc_curve(y_test, y_pred_test)\n",
    "roc_auc = auc(false_positive_rate, true_positive_rate)\n",
    "#calculate other metrics scores\n",
    "accuracy = accuracy_score(y_pred=y_pred_test, y_true=y_test)\n",
    "precision = precision_score(y_pred=y_pred_test, y_true=y_test)\n",
    "recall = recall_score(y_pred=y_pred_test,y_true=y_test)\n",
    "f1score = f1_score(y_true=y_test, y_pred=y_pred_test)\n",
    "\n",
    "#entering tuned metrics score to the dataframe\n",
    "smoteenn_resampled_tuned_metrics.loc['accuracy','LogisticReg'] = accuracy\n",
    "smoteenn_resampled_tuned_metrics.loc['precision','LogisticReg'] = precision\n",
    "smoteenn_resampled_tuned_metrics.loc['recall','LogisticReg'] = recall\n",
    "smoteenn_resampled_tuned_metrics.loc['roc_auc','LogisticReg'] = roc_auc\n",
    "smoteenn_resampled_tuned_metrics.loc['f1','LogisticReg'] = f1score\n",
    "\n",
    "# Print the tuned parameters and score\n",
    "print('Tuned Logistic Regression with SMOTE+ENN Parameters: {}'.format(clf.best_params_))\n",
    "print('Best score is {0:.4f}'.format(clf.best_score_))\n",
    "print('AUC Score: {0:.4f}'.format(roc_auc))\n",
    "print('Accuracy Score: {0:.4f}'.format(accuracy))\n",
    "print('Precision Score: {0:.4f}'.format(precision))\n",
    "print('Recall Score: {0:.4f}'.format(recall))\n",
    "print('f1 score: {0:.4f}'.format(f1score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SMOTE+ENN with Decision Tree Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_base = DecisionTreeClassifier()\n",
    "\n",
    "# Setup the parameters and distributions to sample from: param_dist\n",
    "param_grid = {\"max_depth\": range(1,10),\n",
    "             'max_features': range(1,10)}\n",
    "\n",
    "clf = RandomizedSearchCV(clf_base, param_grid, cv=5, n_iter=7, scoring='f1')\n",
    "\n",
    "clf.fit(X_train_res, y_train_res)\n",
    "\n",
    "#to store the predicted labels\n",
    "y_pred_test = clf.predict(X_test)\n",
    "\n",
    "#to calculate the auc score\n",
    "false_positive_rate, true_positive_rate, thresholds = roc_curve(y_test, y_pred_test)\n",
    "roc_auc = auc(false_positive_rate, true_positive_rate)\n",
    "#calculate other metrics scores\n",
    "accuracy = accuracy_score(y_pred=y_pred_test, y_true=y_test)\n",
    "precision = precision_score(y_pred=y_pred_test, y_true=y_test)\n",
    "recall = recall_score(y_pred=y_pred_test,y_true=y_test)\n",
    "f1score = f1_score(y_true=y_test, y_pred=y_pred_test)\n",
    "\n",
    "#entering tuned metrics score to the dataframe\n",
    "smoteenn_resampled_tuned_metrics.loc['accuracy','DecisionTree'] = accuracy\n",
    "smoteenn_resampled_tuned_metrics.loc['precision','DecisionTree'] = precision\n",
    "smoteenn_resampled_tuned_metrics.loc['recall','DecisionTree'] = recall\n",
    "smoteenn_resampled_tuned_metrics.loc['roc_auc','DecisionTree'] = roc_auc\n",
    "smoteenn_resampled_tuned_metrics.loc['f1','DecisionTree'] = f1score\n",
    "\n",
    "# Print the tuned parameters and score\n",
    "print('Tuned Decision Tree with SMOTE+ENN Parameters: {}'.format(clf.best_params_))\n",
    "print('Best score is {0:.4f}'.format(clf.best_score_))\n",
    "print('AUC Score: {0:.4f}'.format(roc_auc))\n",
    "print('Accuracy Score: {0:.4f}'.format(accuracy))\n",
    "print('Precision Score: {0:.4f}'.format(precision))\n",
    "print('Recall Score: {0:.4f}'.format(recall))\n",
    "print('f1 score: {0:.4f}'.format(f1score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SMOTE+ENN with GaussianNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_base = GaussianNB()\n",
    "\n",
    "# Setup the parameters and distributions to sample from: param_dist\n",
    "param_dist = {'priors':[None]}\n",
    "\n",
    "clf = GridSearchCV(clf_base, param_dist, cv=5, scoring='f1')\n",
    "\n",
    "clf.fit(X_train_res, y_train_res)\n",
    "\n",
    "#to store the predicted labels\n",
    "y_pred_test = clf.predict(X_test)\n",
    "\n",
    "#to calculate the auc score\n",
    "false_positive_rate, true_positive_rate, thresholds = roc_curve(y_test, y_pred_test)\n",
    "roc_auc = auc(false_positive_rate, true_positive_rate)\n",
    "#calculate other metrics scores\n",
    "accuracy = accuracy_score(y_pred=y_pred_test, y_true=y_test)\n",
    "precision = precision_score(y_pred=y_pred_test, y_true=y_test)\n",
    "recall = recall_score(y_pred=y_pred_test,y_true=y_test)\n",
    "f1score = f1_score(y_true=y_test, y_pred=y_pred_test)\n",
    "\n",
    "#entering tuned metrics score to the dataframe\n",
    "smoteenn_resampled_tuned_metrics.loc['accuracy','GaussianNB'] = accuracy\n",
    "smoteenn_resampled_tuned_metrics.loc['precision','GaussianNB'] = precision\n",
    "smoteenn_resampled_tuned_metrics.loc['recall','GaussianNB'] = recall\n",
    "smoteenn_resampled_tuned_metrics.loc['roc_auc','GaussianNB'] = roc_auc\n",
    "smoteenn_resampled_tuned_metrics.loc['f1','GaussianNB'] = f1score\n",
    "\n",
    "# Print the tuned parameters and score\n",
    "print('Tuned GaussianNB with SMOTE+ENN Parameters: {}'.format(clf.best_params_))\n",
    "print('Best score is {0:.4f}'.format(clf.best_score_))\n",
    "print('AUC Score: {0:.4f}'.format(roc_auc))\n",
    "print('Accuracy Score: {0:.4f}'.format(accuracy))\n",
    "print('Precision Score: {0:.4f}'.format(precision))\n",
    "print('Recall Score: {0:.4f}'.format(recall))\n",
    "print('f1 score: {0:.4f}'.format(f1score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SMOTE+ENN with Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_base = RandomForestClassifier()\n",
    "\n",
    "# Setup the parameters and distributions to sample from: param_dist\n",
    "param_dist = {'max_depth': range(1,10),\n",
    "             'max_features': range(1,10),\n",
    "             'n_estimators': [10,50,100,150,200,250,300,350,400,500]}\n",
    "\n",
    "clf = RandomizedSearchCV(clf_base, param_dist, cv=5,n_jobs=8, scoring='f1')\n",
    "\n",
    "clf.fit(X_train_res, y_train_res)\n",
    "\n",
    "#to store the predicted labels\n",
    "y_pred_test = clf.predict(X_test)\n",
    "\n",
    "#to calculate the auc score\n",
    "false_positive_rate, true_positive_rate, thresholds = roc_curve(y_test, y_pred_test)\n",
    "roc_auc = auc(false_positive_rate, true_positive_rate)\n",
    "#calculate other metrics scores\n",
    "accuracy = accuracy_score(y_pred=y_pred_test, y_true=y_test)\n",
    "precision = precision_score(y_pred=y_pred_test, y_true=y_test)\n",
    "recall = recall_score(y_pred=y_pred_test,y_true=y_test)\n",
    "f1score = f1_score(y_true=y_test, y_pred=y_pred_test)\n",
    "\n",
    "#entering tuned metrics score to the dataframe\n",
    "smoteenn_resampled_tuned_metrics.loc['accuracy','RandomForest'] = accuracy\n",
    "smoteenn_resampled_tuned_metrics.loc['precision','RandomForest'] = precision\n",
    "smoteenn_resampled_tuned_metrics.loc['recall','RandomForest'] = recall\n",
    "smoteenn_resampled_tuned_metrics.loc['roc_auc','RandomForest'] = roc_auc\n",
    "smoteenn_resampled_tuned_metrics.loc['f1','RandomForest'] = f1score\n",
    "\n",
    "# Print the tuned parameters and score\n",
    "print('Tuned Random Forest with SMOTE+ENN Parameters: {}'.format(clf.best_params_))\n",
    "print('Best score is {0:.4f}'.format(clf.best_score_))\n",
    "print('AUC Score: {0:.4f}'.format(roc_auc))\n",
    "print('Accuracy Score: {0:.4f}'.format(accuracy))\n",
    "print('Precision Score: {0:.4f}'.format(precision))\n",
    "print('Recall Score: {0:.4f}'.format(recall))\n",
    "print('f1 score: {0:.4f}'.format(f1score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SMOTE+ENN with Support Vector Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_base = svm.SVC(kernel='linear')\n",
    "\n",
    "# Setup the parameters and distributions to sample from: param_dist\n",
    "param_dist = {'C': [0.001,0.01,0.1,1], 'gamma': [0.001,0.01,0.1,1]}\n",
    "\n",
    "clf = RandomizedSearchCV(clf_base, param_dist, cv=5, scoring='f1')\n",
    "\n",
    "clf.fit(X_train_res, y_train_res)\n",
    "\n",
    "#to store the predicted labels\n",
    "y_pred_test = clf.predict(X_test)\n",
    "\n",
    "#to calculate the auc score\n",
    "false_positive_rate, true_positive_rate, thresholds = roc_curve(y_test, y_pred_test)\n",
    "roc_auc = auc(false_positive_rate, true_positive_rate)\n",
    "#calculate other metrics scores\n",
    "accuracy = accuracy_score(y_pred=y_pred_test, y_true=y_test)\n",
    "precision = precision_score(y_pred=y_pred_test, y_true=y_test)\n",
    "recall = recall_score(y_pred=y_pred_test,y_true=y_test)\n",
    "f1score = f1_score(y_true=y_test, y_pred=y_pred_test)\n",
    "\n",
    "#entering tuned metrics score to the dataframe\n",
    "smoteenn_resampled_tuned_metrics.loc['accuracy','SVM'] = accuracy\n",
    "smoteenn_resampled_tuned_metrics.loc['precision','SVM'] = precision\n",
    "smoteenn_resampled_tuned_metrics.loc['recall','SVM'] = recall\n",
    "smoteenn_resampled_tuned_metrics.loc['roc_auc','SVM'] = roc_auc\n",
    "smoteenn_resampled_tuned_metrics.loc['f1','SVM'] = f1score\n",
    "\n",
    "# Print the tuned parameters and score\n",
    "print('Tuned SVM with SMOTE+ENN Parameters: {}'.format(clf.best_params_))\n",
    "print('Best score is {0:.4f}'.format(clf.best_score_))\n",
    "print('AUC Score: {0:.4f}'.format(roc_auc))\n",
    "print('Accuracy Score: {0:.4f}'.format(accuracy))\n",
    "print('Precision Score: {0:.4f}'.format(precision))\n",
    "print('Recall Score: {0:.4f}'.format(recall))\n",
    "print('f1 score: {0:.4f}'.format(f1score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "100*smoteenn_resampled_tuned_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "100*smoteenn_resampled_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After tuning the hyperparameter of all different model, we can see that the f1 score for Decision Tree had decreased after hyperparameter tuning, and the f1 score for Random Forest has improved and increased. Logistic Regression, GaussianNB, and SVM has not changed even after tuning. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot a horizontal bar plot for the metrics of different classifier\n",
    "fig, ax = plt.subplots(figsize=(15,8))\n",
    "smoteenn_resampled_metrics.loc['f1'].plot(kind='barh', ax=ax)\n",
    "ax.axvline(max(smoteenn_resampled_metrics.loc['f1']))\n",
    "ax.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic Regression has the best f1 score compared to all other classification model, so I will choose Logistic Regression for this project. Since the F1-score is significantly better than a random classifier, I would recommend this Logistic Regression model to predict credit card default. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
